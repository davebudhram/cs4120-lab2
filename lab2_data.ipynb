{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Data!\n",
    "\n",
    "10/16/2023, Felix Muzny, Nidhi Bodar, Harshitha Somala, Ankit Ramakrishnan\n",
    "\n",
    "Agenda\n",
    "------\n",
    "+ your data set\n",
    "+ balanced/imbalanced data sets\n",
    "    - oversampling correction\n",
    "+ training a Logistic Regression classifier\n",
    "    - evaluating classifier f_measure\n",
    "+ training a Neural Net w/ Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0: Who is in your group?\n",
    "\n",
    "Dave Budhram, Akshay Dupuguntla, Sunny Huang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1 : Getting familiar with your data\n",
    "---\n",
    "\n",
    "For this task, you'll load in your data, gather some statistics about it, and work on some featurization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if you'd like to use pandas, you can, but you aren't required to\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# for f_measure, word_tokenize\n",
    "import nltk\n",
    "import nltk.corpus as corpus\n",
    "# you can also use sklearn.metrics.f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# this function will help split our data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feel free to use these fancier data structures\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your data file should be in the same directory as this notebook\n",
    "# take a look at this file to see what it looks like and how it is structured\n",
    "# we've cleaned this dataset for you so you won't see any missing values\n",
    "\n",
    "# The dataset contains spam and non-spam emails\n",
    "# Labels: 0 - non-spam, 1 - spam\n",
    "DATAFILE = \"spam_or_not_spam.csv\"\n",
    "\n",
    "# read the data into a pandas dataframe or into numpy arrays or into a list of lists\n",
    "spam_df = pd.read_csv(DATAFILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>martin a posted tassos papadopoulos the greek ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man threatens explosion in moscow thursday aug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klez the virus that won t die already the most...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in adding cream to spaghetti carbonara which ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               email  label\n",
       "0   date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...      0\n",
       "1  martin a posted tassos papadopoulos the greek ...      0\n",
       "2  man threatens explosion in moscow thursday aug...      0\n",
       "3  klez the virus that won t die already the most...      0\n",
       "4   in adding cream to spaghetti carbonara which ...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first few lines of your data to make sure it looks like what you expect\n",
    "# make sure not to overwhelm yourself/your reader by printing too much data!\n",
    "spam_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2997\n",
      "{0, 1}\n",
      "0    2500\n",
      "1     497\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO-DO\n",
    "# Display the following\n",
    "# Size of dataset\n",
    "# Number of unique classes in the dataset \n",
    "# Display all the unique classes\n",
    "# Number of samples for each class\n",
    "\n",
    "# Size of dataset\n",
    "print(len(spam_df))\n",
    "\n",
    "# Unique classes in the dataset (find this programmatically!)\n",
    "print(set(spam_df['label']))\n",
    "\n",
    "# Number of samples for each class\n",
    "print(spam_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer the following:**\n",
    "1. Is the dataset imbalanced? Yes, there are 5 times more emails with label 0 than 1\n",
    "\n",
    "__Imbalanced__ datasets: Imbalanced datasets refers to the data in which the targeted classes are not equally distributed. In other words, number of data samples of one class significantly outnumbers the data samples of other class. This significant difference between datasamples of classes can lead to biased models favoring the majority class and results in poor spam detection and compromised accuracy. It is always essential to balance the dataset to tackle this issue.\n",
    "\n",
    "The following are some simple techniques to handle imbalance datasets:\n",
    "- Random Oversampling (randomly select minority class examples with replacement and add to training data)\n",
    "- Random Undersampling (randomly select majority class examples and delete from the training data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Random Oversampling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to handle oversampling (also known as upsampling)\n",
    "# implement EITHER the pandas version OR the list version\n",
    "# you don't need to implement both\n",
    "# delete whichever one you don't want to use\n",
    "def oversample_minority_class_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Oversamples or upsamples the minority class in the given DataFrame to balance the class distribution.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame containing \n",
    "        'email' column representing feature\n",
    "        'label' column representing the class labels.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame \n",
    "        Concatenated dataframe - original majority samples + original minority samples + upsampled minority samples\n",
    "        DataFrame with the minority class oversampled to match the size of the majority class.\n",
    "    \"\"\"\n",
    "    ## TO-DO\n",
    "\n",
    "    # Identify the majority and minority class labels from the 'label' column in the DataFrame.\n",
    "    counts = df['label'].value_counts()\n",
    "    majority = counts.idxmax()\n",
    "    minority = counts.idxmin()\n",
    "    # Separate the majority and minority classes into different DataFrames based on their labels.\n",
    "    df_majority = df[df['label'] == majority]\n",
    "    df_minority = df[df['label'] == minority]\n",
    "\n",
    "    # Determine the number of samples in the minority and majority classes.\n",
    "    majority_count = len(df_majority)\n",
    "    minority_count = len(df_minority)\n",
    "\n",
    "    # Calculate the number of samples to be added to upsample the minority class.\n",
    "    minority_needed = majority_count - minority_count\n",
    "\n",
    "    # Randomly select samples (with replacement) from the minority class to create upsampled data.\n",
    "    # create a dataset with upsampled minority class\n",
    "    # Hint: Can use random.choices\n",
    "    seed_value = 42\n",
    "    random.seed(seed_value)\n",
    "    upsampled_minority_class = random.choices(df_minority.to_records(index=False), k=minority_needed )\n",
    "    # Convert the upsampled minority class data back to a DataFrame\n",
    "    df_upsampled_minority_class = pd.DataFrame.from_records(upsampled_minority_class, columns=df.columns)\n",
    "\n",
    "    # Concatenate the upsampled minority class dataframe with the original majority class and minority class.\n",
    "    upsampled_with_minority_df = pd.concat([df_minority, df_upsampled_minority_class])\n",
    "    upsampled_df = pd.concat([df_majority, upsampled_with_minority_df])\n",
    "    # Return the resulting DataFrame with balanced class distribution.\n",
    "    return upsampled_df\n",
    "\n",
    "\n",
    "# function to handle oversampling (also known as upsampling)\n",
    "def oversample_minority_class(data: list) -> list:\n",
    "    \"\"\"\n",
    "    Oversamples or upsamples the minority class in the given DataFrame to balance the class distribution.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : list\n",
    "        Input data containing lists with two elements:\n",
    "        the first is the email and the second is the label\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        list of lists - original majority samples + original minority samples + upsampled minority samples\n",
    "        DataFrame with the minority class oversampled to match the size of the majority class.\n",
    "    \"\"\"\n",
    "    ## TO-DO\n",
    "\n",
    "    # Identify the majority and minority class labels\n",
    "    \n",
    "    # Separate the majority and minority classes into different groups\n",
    "    \n",
    "    \n",
    "    # Determine the number of samples in the minority and majority classes.\n",
    "    \n",
    "    # Calculate the number of samples to be added to upsample the minority class.\n",
    "\n",
    "    # Randomly select samples (with replacement) from the minority class to create upsampled data.\n",
    "    # create a dataset with upsampled minority class\n",
    "    # Hint: Can use random.choices with the k= optional parameter\n",
    "\n",
    "    # Return the resulting list of lists with balanced class distribution.\n",
    "    \n",
    "\n",
    "# call and test your function\n",
    "# keep your original data around too! You'll need it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "{0, 1}\n",
      "0    2500\n",
      "1    2500\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO-DO\n",
    "# Display the following\n",
    "# Size of oversampled dataset\n",
    "# Number of samples for each class in your oversampled dataset\n",
    "# df = pd.read_csv(DATAFILE)\n",
    "spam_oversampled_df = oversample_minority_class_df(spam_df)\n",
    "# Size of dataset\n",
    "print(len(spam_oversampled_df))\n",
    "# Unique classes\n",
    "print(set(spam_oversampled_df['label']))\n",
    "# Number of samples for each class\n",
    "print(spam_oversampled_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Train a Logistic Regression Classifier\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO\n",
    "# your first step before training a Logistic Regression classifier\n",
    "# is to featurize your data.\n",
    "\n",
    "# scikit-learn LogisticRegression and keras neural networks like data in\n",
    "# the format:\n",
    "# X: a matrix of shape (num_samples, num_features)\n",
    "# y: a vector of shape (num_samples,)\n",
    "\n",
    "# start by defining some words as your features — you'll be using these\n",
    "# as a mini bag of words. Choose up to 10 words that you think might be\n",
    "# indicative of spam emails. You can look at the data to get some ideas.\n",
    "WORDS = ['money', 'card', 'free', 'cash', 'refund', 'credit', 'best', 'save', 'now', 'congratulations']\n",
    "\n",
    "def featurize(X_train):\n",
    "    # implement this function to featurize your data\n",
    "    # use nltk.word_tokenize to tokenize the emails\n",
    "    array = []\n",
    "    for sentence in X_train:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        counter = Counter(tokens)\n",
    "        spam_words = []\n",
    "        for word in WORDS:\n",
    "            if word not in counter:\n",
    "                spam_words.append(0)\n",
    "            else:\n",
    "                spam_words.append(counter[word])\n",
    "        array.append(spam_words)\n",
    "    return array\n",
    "    \n",
    "    \n",
    "# make sure you've successfully featurized your data\n",
    "\n",
    "# the shape should be (num_samples, num_features) where num_features\n",
    "# is the length of your word list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = featurize(spam_oversampled_df['email'])\n",
    "labels = spam_oversampled_df['label']\n",
    "# this function will put 70% of your data into the training set and 30% into the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your model\n",
    "oversampled_model = LogisticRegression()\n",
    "oversampled_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how good your model is by printing the f1 score\n",
    "lr_preds = oversampled_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7286666666666667\n"
     ]
    }
   ],
   "source": [
    "# what is the distribution of your predictions?\n",
    "# is your model predicting all 0s or 1s?\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, lr_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 0 0]\n",
      "Counter({0: 931, 1: 569})\n"
     ]
    }
   ],
   "source": [
    "print(lr_preds)\n",
    "count = Counter(lr_preds)\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the __original__ data, what label distribution does your LogisticRegression classifier give? See below\n",
    "1. Using the __oversampled/upsampled__ data, what label distribution does your LogisticRegression classifier give? See above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 840, 1: 60})\n"
     ]
    }
   ],
   "source": [
    "# split your data into train and test sets\n",
    "features = featurize(spam_df['email'])\n",
    "labels = spam_df['label']\n",
    "# this function will put 70% of your data into the training set and 30% into the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# train your model\n",
    "spam_model = LogisticRegression()\n",
    "spam_model.fit(X_train, y_train)\n",
    "lr_preds = spam_model.predict(X_test)\n",
    "count = Counter(lr_preds)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70273435 -0.05135297  0.93570958  0.95170556 -1.12075203  1.27362274\n",
      "  -0.07438184  0.78240272 -0.08678875  1.1951754 ]]\n",
      "credit\n"
     ]
    }
   ],
   "source": [
    "# finally, take a look at the model's coefficients (weights) (model.coef_)\n",
    "# which word is the most important for predicting spam?\n",
    "coefficients = oversampled_model.coef_\n",
    "abs_coefficients = np.array([abs(x) for x in coefficients])\n",
    "print(coefficients)\n",
    "print(WORDS[abs_coefficients.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6175562  -0.18227245  0.90549302  1.00808619 -0.65278403  1.50960816\n",
      "  -0.39014197  0.71379644  0.01539242  1.3880844 ]]\n",
      "credit\n"
     ]
    }
   ],
   "source": [
    "coefficients = spam_model.coef_\n",
    "abs_coefficients = np.array([abs(x) for x in coefficients])\n",
    "print(coefficients)\n",
    "print(WORDS[abs_coefficients.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the most important word when using the __original__ data? Credit\n",
    "4. What is the most important word when using the __oversampled/upsampled__ data? Credit\n",
    "5. What do the relative weights of the model when using the original vs. the oversampled data tell you about the effects of using oversampled data on this model? (1 - 2 sentences) __YOUR ANSWER HERE__\n",
    "6. We didn't implement it today, but we can also __downsample/undersample__ in which we reduce the size of the majority class in our data. What effects do you think that this would have? (1 - 2 sentences) __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4: Training a Neural Net\n",
    "---\n",
    "\n",
    "Finally, we'll train and evaluate a neural net using the same data as your Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_35 (Dense)            (None, 4)                 44        \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49 (196.00 Byte)\n",
      "Trainable params: 49 (196.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create your model\n",
    "# (take a look at lecture notebook 9 for an example of creating a keras neural network)\n",
    "df = pd.read_csv(DATAFILE)\n",
    "df = oversample_minority_class_df(df)\n",
    "features = featurize(df['email'])\n",
    "labels = df['label']\n",
    "# this function will put 70% of your data into the training set and 30% into the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "# hidden layer\n",
    "# you can play around with different activation functions\n",
    "model.add(Dense(units=4, activation='relu', input_dim=10))\n",
    "\n",
    "# output layer\n",
    "# activation function is our classification function\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# configure the learning process\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary() #will tell you about your model\n",
    "# call compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many __hidden__ layers does your network have? 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 0s 592us/step - loss: 0.6619 - accuracy: 0.6560\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 0s 553us/step - loss: 0.6533 - accuracy: 0.6660\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 0s 563us/step - loss: 0.6447 - accuracy: 0.6734\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 0s 554us/step - loss: 0.6365 - accuracy: 0.6817\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 0s 578us/step - loss: 0.6288 - accuracy: 0.6883\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 0s 584us/step - loss: 0.6209 - accuracy: 0.6926\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 0s 615us/step - loss: 0.6136 - accuracy: 0.6937\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 0s 719us/step - loss: 0.6077 - accuracy: 0.6937\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 0s 596us/step - loss: 0.6027 - accuracy: 0.6949\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 0s 558us/step - loss: 0.5985 - accuracy: 0.6960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2939f4c40>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your model\n",
    "# if you get this error:\n",
    "# ValueError: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})\n",
    "# this means that your inputs need to be numpy arrays\n",
    "\n",
    "# some useful parameters to the keras model.fit function:\n",
    "# epochs: how many times you want to go through your training data\n",
    "# batch_size: how many examples to look at before updating the weights in your network\n",
    "# verbose: whether or not you want to see the training progress\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 564us/step\n",
      "[[0.41495636]\n",
      " [0.42940155]\n",
      " [0.75322706]\n",
      " ...\n",
      " [0.41495636]\n",
      " [0.41495636]\n",
      " [0.41495636]]\n",
      "0.6734059097978227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# see how good your model is (use the `model.predict(test_data)` function)\n",
    "# print out the f1 score\n",
    "preds = model.predict(np.array(X_test))\n",
    "print(preds)\n",
    "preds = [x[0] for x in preds]\n",
    "preds = [1 if x > .5 else 0 for x in preds]\n",
    "\n",
    "print(f1_score(np.array(y_test), preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the output of the `keras` function `model.predict(test_data)`? Is it the same as your `sklearn` `LogisticRegression` model's `model.predict(test_data)` function? The distributions are very similar. With the keras we have 0: 946, 1: 554 and with LogisticRegression we have 0: 931, 1: 569"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 946, 1: 554})\n"
     ]
    }
   ],
   "source": [
    "# what is the distribution of your predictions?\n",
    "# is your model predicting all 0s or 1s?\n",
    "\n",
    "count = Counter(preds)\n",
    "print(count)\n",
    "\n",
    "# compare and contrast your LR predictions with your NN predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using the __original__ data, what label distribution does your neural classifier give? __YOUR ANSWER HERE__ ({0.0: 784, 1.0: 116})\n",
    "    1. What is the overlap with the Logistic Regression guesses (# both models agree on)? __YOUR ANSWER HERE__ ({True: 885, False: 15})\n",
    "4. Using the __oversampled/upsampled__ data, what label distribution does your neural classifier give? __YOUR ANSWER HERE__ (({0.0: 921, 1.0: 579}))\n",
    "    1. What is the overlap with the Logistic Regression guesses (# both models agree on)? __YOUR ANSWER HERE__ (1493 agree, 7 disagree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Make sure to clear your kernel and run your notebook from top to bottom, ensuring there are no errors, before turning it in!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
